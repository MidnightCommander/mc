#!/usr/bin/env python3
#
#  Midnight Commander compatible EXTFS for accessing Amazon Web Services S3.
#  Converted to boto3 from original boto version by Jakob Kemi <jakob.kemi@gmail.com> 2009
#
#  This program is free software: you can redistribute it and/or modify
#  it under the terms of the GNU General Public License as published by
#  the Free Software Foundation, either version 3 of the License, or
#  (at your option) any later version.
#
#  This program is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
#  You should have received a copy of the GNU General Public License
#  along with this program.  If not, see <http://www.gnu.org/licenses/>.
#

import sys
import os
import time
import re
import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import queue

import boto3
from botocore.exceptions import ClientError, NoCredentialsError

# Get settings from environment
USER = os.getenv('USER', '0')
AWS_ACCESS_KEY_ID = os.getenv('AWS_ACCESS_KEY_ID')
AWS_SECRET_ACCESS_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')
AWS_DEFAULT_REGION = os.getenv('AWS_DEFAULT_REGION', 'us-east-1')
S3LOCATION = os.getenv('MCVFS_EXTFS_S3_LOCATION', 'us-east-1')
DEBUGFILE = os.getenv('MCVFS_EXTFS_S3_DEBUGFILE', '~/mc-s3.log')
DEBUGLEVEL = os.getenv('MCVFS_EXTFS_S3_DEBUGLEVEL', 'WARNING')

if not AWS_ACCESS_KEY_ID or not AWS_SECRET_ACCESS_KEY:
    sys.stderr.write('Missing AWS_ACCESS_KEY_ID or AWS_SECRET_ACCESS_KEY environment variables.\n')
    sys.exit(1)

# Setup logging
if DEBUGFILE:
    import logging
    logging.basicConfig(
        filename=os.path.expanduser(DEBUGFILE),
        level=logging.DEBUG,
        format='%(asctime)s %(levelname)s %(message)s')
    logging.getLogger('boto3').setLevel(getattr(logging, DEBUGLEVEL))
    logging.getLogger('botocore').setLevel(getattr(logging, DEBUGLEVEL))
else:
    class Void(object):
        def __getattr__(self, attr):
            return self
        def __call__(self, *args, **kw):
            return self
    logging = Void()

logger = logging.getLogger('s3extfs')


def __fix_io_encoding(last_resort_default='UTF-8'):
    """
    The following code is needed to work with non-ASCII characters in filenames.
    We're trying hard to detect the system encoding.
    """
    import codecs
    import locale
    for var in ('stdin', 'stdout', 'stderr'):
        if getattr(sys, var).encoding is None:
            enc = None
            if enc is None:
                try:
                    enc = locale.getpreferredencoding()
                except:
                    pass
            if enc is None:
                try:
                    enc = sys.getfilesystemencoding()
                except:
                    pass
            if enc is None:
                try:
                    enc = sys.stdout.encoding
                except:
                    pass
            if enc is None:
                enc = last_resort_default
            setattr(sys, var, codecs.getwriter(enc)(getattr(sys, var), 'strict'))


__fix_io_encoding()


def threadmap(fun, iterable, maxthreads=16):
    """
    Threaded version of builtin method map using ThreadPoolExecutor.
    """
    items = list(iterable)
    if len(items) < 2:
        return list(map(fun, items))
    
    results = []
    with ThreadPoolExecutor(max_workers=min(len(items), maxthreads)) as executor:
        future_to_item = {executor.submit(fun, item): item for item in items}
        for future in as_completed(future_to_item):
            try:
                result = future.result()
                results.append(result)
            except Exception as exc:
                logger.error(f'Generated an exception: {exc}')
                raise exc
    
    return results


logger.debug('started')

# Create boto3 session and clients
session = boto3.Session(
    aws_access_key_id=AWS_ACCESS_KEY_ID,
    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
    region_name=AWS_DEFAULT_REGION
)

# Global S3 client
s3_client = session.client('s3')
s3_resource = session.resource('s3')


def get_bucket_region(bucket_name):
    """
    Get the region of a specific bucket.
    """
    try:
        response = s3_client.get_bucket_location(Bucket=bucket_name)
        region = response['LocationConstraint']
        # get_bucket_location returns None for us-east-1
        return region if region else 'us-east-1'
    except ClientError as e:
        logger.error(f'Error getting bucket region: {e}')
        return AWS_DEFAULT_REGION


def get_regional_client(region):
    """
    Get an S3 client for a specific region.
    """
    return session.client('s3', region_name=region)


logger.debug('argv: ' + str(sys.argv))
try:
    cmd = sys.argv[1]
    args = sys.argv[2:]
except:
    sys.stderr.write('This program should be called from within MC\n')
    sys.exit(1)


def handle_client_error(msg, error):
    """
    Handle boto3 ClientError exceptions.
    """
    error_msg = f'{msg}, reason: {error}'
    logger.error(error_msg)
    sys.stderr.write(error_msg + '\n')
    sys.exit(1)


#
# Lists all S3 contents
#
if cmd == 'list':
    if len(args) > 0:
        path = args[0]
    else:
        path = ''

    logger.info('list')

    try:
        # List all buckets
        response = s3_client.list_buckets()
        buckets = response['Buckets']
    except ClientError as e:
        handle_client_error('Unable to list buckets', e)

    # Import python timezones (pytz)
    try:
        import pytz
    except:
        logger.warning('Missing pytz module, timestamps will be off')
        # A fallback UTC tz stub
        class pytzutc(datetime.tzinfo):
            def __init__(self):
                datetime.tzinfo.__init__(self)
                self.utc = self
                self.zone = 'UTC'
            def utcoffset(self, dt):
                return datetime.timedelta(0)
            def tzname(self, dt):
                return "UTC"
            def dst(self, dt):
                return datetime.timedelta(0)
        pytz = pytzutc()

    # Find timezone
    def getGuessedTimezone():
        # 1. check TZ env. var
        try:
            tz = os.getenv('TZ', '')
            return pytz.timezone(tz)
        except:
            pass
        # 2. check if /etc/timezone exists (Debian at least)
        try:
            if os.path.isfile('/etc/timezone'):
                tz = open('/etc/timezone', 'r').readline().strip()
                return pytz.timezone(tz)
        except:
            pass
        # 3. check if /etc/localtime is a _link_ to something useful
        try:
            if os.path.islink('/etc/localtime'):
                link = os.readlink('/etc/localtime')
                tz = '/'.join(link.split(os.path.sep)[-2:])
                return pytz.timezone(tz)
        except:
            pass
        # 4. use time.tzname which will probably be wrong by an hour 50% of the time.
        try:
            return pytz.timezone(time.tzname[0])
        except:
            pass
        # 5. use plain UTC ...
        return pytz.utc

    tz = getGuessedTimezone()
    logger.debug('Using timezone: ' + tz.zone)

    def convDate(aws_datetime):
        """
        Convert AWS datetime to local time format.
        """
        if isinstance(aws_datetime, datetime.datetime):
            # boto3 returns datetime objects directly
            dt = aws_datetime.replace(tzinfo=pytz.utc)
            return dt.astimezone(tz).strftime('%m-%d-%Y %H:%M')
        else:
            # Fallback for string format
            expr = re.compile(r'^(\d{4})-(\d{2})-(\d{2})T(\d{2}):(\d{2}):(\d{2})\.\d{3}Z$')
            m = expr.match(str(aws_datetime))
            if m:
                ye, mo, da, ho, mi, se = list(map(int, m.groups()))
                dt = datetime.datetime(ye, mo, da, ho, mi, se, tzinfo=pytz.utc)
                return dt.astimezone(tz).strftime('%m-%d-%Y %H:%M')
            return str(aws_datetime)

    def bucketList(bucket_info):
        bucket_name = bucket_info['Name']
        try:
            # Get bucket region and create regional client
            region = get_bucket_region(bucket_name)
            regional_client = get_regional_client(region)
            
            # List objects in bucket
            paginator = regional_client.get_paginator('list_objects_v2')
            page_iterator = paginator.paginate(Bucket=bucket_name)
            
            totsz = 0
            mostrecent = datetime.datetime(1970, 1, 1, tzinfo=pytz.utc)
            ret = []
            
            for page in page_iterator:
                if 'Contents' in page:
                    for obj in page['Contents']:
                        key_name = obj['Key']
                        if key_name.endswith('/'):
                            # Skip directory markers
                            continue
                        
                        size = obj['Size']
                        last_modified = obj['LastModified']
                        
                        mostrecent = max(mostrecent, last_modified)
                        datetime_str = convDate(last_modified)
                        
                        ret.append('%10s %3d %-8s %-8s %d %s %s\n' % (
                            '-rw-r--r--', 1, USER, USER, size, datetime_str, 
                            bucket_name + '/' + key_name)
                        )
                        totsz += size

            datetime_str = convDate(mostrecent)
            sys.stdout.write('%10s %3d %-8s %-8s %d %s %s\n' % (
                'drwxr-xr-x', 1, USER, USER, totsz, datetime_str, bucket_name)
            )
            for line in ret:
                sys.stdout.write(line)
                
        except ClientError as e:
            logger.error(f'Error listing bucket {bucket_name}: {e}')

    threadmap(bucketList, buckets)

#
# Fetch file from S3
#
elif cmd == 'copyout':
    archivename = args[0]
    storedfilename = args[1]
    extractto = args[2]

    bucket, key = storedfilename.split('/', 1)
    logger.info('copyout bucket: %s, key: %s' % (bucket, key))

    try:
        # Get bucket region and create regional client
        region = get_bucket_region(bucket)
        regional_client = get_regional_client(region)
        
        # Download file
        regional_client.download_file(bucket, key, extractto)
        
    except ClientError as e:
        handle_client_error(f'Unable to fetch key "{key}"', e)

#
# Upload file to S3
#
elif cmd == 'copyin':
    archivename = args[0]
    storedfilename = args[1]
    sourcefile = args[2]

    bucket, key = storedfilename.split('/', 1)
    logger.info('copyin bucket: %s, key: %s' % (bucket, key))

    try:
        # Get bucket region and create regional client
        region = get_bucket_region(bucket)
        regional_client = get_regional_client(region)
        
        # Upload file
        regional_client.upload_file(sourcefile, bucket, key)
        
    except ClientError as e:
        handle_client_error(f'Unable to upload key "{key}"', e)

#
# Remove file from S3
#
elif cmd == 'rm':
    archivename = args[0]
    storedfilename = args[1]

    bucket, key = storedfilename.split('/', 1)
    logger.info('rm bucket: %s, key: %s' % (bucket, key))

    try:
        # Get bucket region and create regional client
        region = get_bucket_region(bucket)
        regional_client = get_regional_client(region)
        
        # Delete object
        regional_client.delete_object(Bucket=bucket, Key=key)
        
    except ClientError as e:
        handle_client_error(f'Unable to remove key "{key}"', e)

#
# Create directory (bucket)
#
elif cmd == 'mkdir':
    archivename = args[0]
    dirname = args[1]

    logger.info('mkdir dir: %s' % (dirname))
    if '/' in dirname:
        logger.warning('skipping mkdir')
        pass
    else:
        bucket = dirname
        try:
            # Create bucket in specified location
            if S3LOCATION == 'us-east-1':
                # us-east-1 doesn't need LocationConstraint
                s3_client.create_bucket(Bucket=bucket)
            else:
                s3_client.create_bucket(
                    Bucket=bucket,
                    CreateBucketConfiguration={'LocationConstraint': S3LOCATION}
                )
        except ClientError as e:
            handle_client_error(f'Unable to create bucket "{bucket}"', e)

#
# Remove directory (bucket)
#
elif cmd == 'rmdir':
    archivename = args[0]
    dirname = args[1]

    logger.info('rmdir dir: %s' % (dirname))
    if '/' in dirname:
        logger.warning('skipping rmdir')
        pass
    else:
        bucket = dirname
        try:
            # Delete bucket
            s3_client.delete_bucket(Bucket=bucket)
        except ClientError as e:
            handle_client_error(f'Unable to delete bucket "{bucket}"', e)

#
# Run from S3
#
elif cmd == 'run':
    archivename = args[0]
    storedfilename = args[1]
    arguments = args[2:]

    bucket, key = storedfilename.split('/', 1)
    logger.info('run bucket: %s, key: %s' % (bucket, key))

    os.execv(storedfilename, arguments)
else:
    logger.error('unhandled, bye')
    sys.exit(1)

logger.debug('command handled')
sys.exit(0)
